---
title: "Chapter 9"
author: "Julin Maloof"
date: "2024-01-19"
output: 
  html_document: 
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Set up

```{r}
library(tidymodels)
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)
  
lm_model <- linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_wflow, ames_train)
```

## 9.1 Performance metrics and inference

Advise to assess prediction accuracy even if your focus in inference.  Need to know model fidelity to the data.

## 9.2 Regression metrics

practice with predictions on the ames test set:

```{r}
ames_test_res <- predict(lm_fit, new_data = ames_test %>% select(-Sale_Price))
ames_test_res
```
 
 add in the observed values:
 
```{r}
ames_test_res <- bind_cols(ames_test_res, ames_test %>% select(Sale_Price))
ames_test_res
```

plot observed vs predicted 
```{r}
ggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) + 
  # Create a diagonal line:
  geom_abline(lty = 2) + 
  geom_point(alpha = 0.5) + 
  labs(y = "Predicted Sale Price (log10)", x = "Sale Price (log10)") +
  # Scale and size the x- and y-axis uniformly:
  coord_obs_pred()
```

calculate RMSE

```{r}
rmse(ames_test_res, truth = Sale_Price, estimate = .pred)

```

can use a metric set to look at multiple metrics

```{r}
ames_metrics <- metric_set(rmse, rsq, mae)
ames_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)
```

NB don't need adj.R2 if working with a test set

## 9.3 Binary Classification Metrics

```{r}
data(two_class_example)
tibble(two_class_example)
```

```{r}
# A confusion matrix: 
conf_mat(two_class_example, truth = truth, estimate = predicted)
```


```{r}
# Accuracy:
accuracy(two_class_example, truth, predicted)
```
Accuracy is correct / total


```{r}
# Matthews correlation coefficient:
mcc(two_class_example, truth, predicted)
```


```{r}
# F1 metric:
f_meas(two_class_example, truth, predicted)

```
F1 is correct class 1 / (correct class 1 + (incorrect class 1 + incorrect class 2) / 2))

```{r}
# Combining these three classification metrics together
classification_metrics <- metric_set(accuracy, mcc, f_meas)
classification_metrics(two_class_example, truth = truth, estimate = predicted)

```

```{r}
two_class_curve <- roc_curve(two_class_example, truth, Class1)
two_class_curve
```


```{r}
roc_auc(two_class_example, truth, Class1)

```

```{r}
autoplot(two_class_curve)
```

## 9.4 Multiclass classification metrics

```{r}
data(hpc_cv)
tibble(hpc_cv)
```

```{r}
conf_mat(hpc_cv, obs, pred)
```


```{r}
accuracy(hpc_cv, obs, pred)
#> # A tibble: 1 × 3
#>   .metric  .estimator .estimate
#>   <chr>    <chr>          <dbl>
#> 1 accuracy multiclass     0.709

mcc(hpc_cv, obs, pred)
#> # A tibble: 1 × 3
#>   .metric .estimator .estimate
#>   <chr>   <chr>          <dbl>
#> 1 mcc     multiclass     0.515
```

### multiclass sensitivity analysis
There are various ways of computing this:


```{r}
sensitivity(hpc_cv, obs, pred, estimator = "macro")
#> # A tibble: 1 × 3
#>   .metric     .estimator .estimate
#>   <chr>       <chr>          <dbl>
#> 1 sensitivity macro          0.560
sensitivity(hpc_cv, obs, pred, estimator = "macro_weighted")
#> # A tibble: 1 × 3
#>   .metric     .estimator     .estimate
#>   <chr>       <chr>              <dbl>
#> 1 sensitivity macro_weighted     0.709
sensitivity(hpc_cv, obs, pred, estimator = "micro")
#> # A tibble: 1 × 3
#>   .metric     .estimator .estimate
#>   <chr>       <chr>          <dbl>
#> 1 sensitivity micro          0.709
```

```{r}
roc_auc(hpc_cv, obs, VF, F, M, L)

```

```{r}
roc_auc(hpc_cv, obs, VF, F, M, L, estimator = "macro_weighted")

```

### grouped analysis
e.g. for different folds

```{r}
hpc_cv %>% 
  group_by(Resample) %>% 
  accuracy(obs, pred)
```


also for ROC curve plotting:

```{r}
# Four 1-vs-all ROC curves for each fold
hpc_cv %>% 
  group_by(Resample) %>% 
  roc_curve(obs, VF, F, M, L) %>% 
  autoplot()
```

