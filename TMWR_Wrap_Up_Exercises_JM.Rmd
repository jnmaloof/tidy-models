---
title: "TMWR_Final_Exercises"
author: "Julin Maloof"
date: "2024-07-07"
output: 
  html_document: 
    keep_md: true
---

```{r}
library(tidyverse)
library(GGally)
library(tidymodels)
tidymodels_prefer()
library(finetune)
library(doMC)
registerDoMC(cores = 7)
```

## Things to practice

* 11 resampling
* 12-14 tuning
* 15 many models
* 18 explaining models
* 19 should you trust your predictions
* 20 ensembles

## Load the data

```{r}
rides <- read_csv("ride_data_smaller.csv.gz")
glimpse(rides)
summary(rides)
```


```{r}
rides %>% ggplot(aes(x=heart_rate)) + geom_histogram()
```


```{r, eval=FALSE}
rides %>% slice_sample(n=1000) %>% 
  select(heart_rate, everything(), -name, -timestamp, -date, -altitude) %>%
  ggpairs(progress = FALSE)
```

## Ex 1.  Training/test/cross validation split
Set.seed to 707 and make a 75/25 training/test split.  Make a v=5 cross validation set.  Should you be grouping by anything?

```{r}
set.seed(707)
rides_split <- group_initial_split(rides, group = name)
#rides_split <- slice_sample(rides, prop=0.1, by = name) %>% group_initial_split(group = name)
rides_train <- training(rides_split)
rides_test <- testing(rides_split)
rides_folds <- group_vfold_cv(rides_train, group = name, v=5)
```

## Ex 2. Create some recipes

Create a recipe `rec_simple` that specifies your outcome variable `heart_rate` and predictors (everthing but `name`, `date`, and `timestamp`)

Create a recipe `rec_normal` that normalizes (centers and scales) all predictors

Check to make sure your recipes are working as expected

```{r}
rec_simple <- recipe(heart_rate ~ ., data = rides_train) %>%
  step_rm(name, date, timestamp)

rec_simple
```


```{r}
rec_normal <- rec_simple %>%
  step_normalize(all_predictors())

rec_normal
```

```{r}
rec_simple %>% prep() %>% bake(rides_train) %>% head()
```

```{r}
rec_normal %>% prep() %>% bake(rides_train) %>% summary()
```

## Ex 3 Create two model specifications

Create a model specification `spec_lm_pen` for a penalized regression (hint see `?details_linear_reg_glmnet` for help).  Set the 2 hyperparameters for tuning.

Create a second model specification `spec_rf` for a random forest regression using ranger (see `?details_rand_forest_ranger`).  Set mtry and min_n for tuning

```{r}
spec_lm_pen <- linear_reg(penalty = tune(), mixture = tune(), engine = "glmnet")
```

```{r}
spec_rf <- rand_forest(mtry=tune(), min_n=tune(), mode = "regression", engine = "ranger")
```

## Ex 4, Workflow + grid tune

Create a workflow that includes the `rec_simple`` and the `spec_lm_pen` model specification.   (Note that while penalized regression is best with normalized predictors, it does this conversion by default, do we can just use the simple recipe for it).

Use the v-fold resampling to fit models and tune the hyper parameters using a grid search with a grid size of 10 (the default).  You'll want to set up parallel processing for this.  How long does it take?

I recommend leaving `save_pred = FALSE` and `save_workflow = FALSE` (these are the defaults).  This is contrary to some of the scripts in the book, but I think Rstudio stays happier with these settings.

Plot the results and also print a table of the best hyperparameters

```{r}
wf1 <- workflow(preprocessor = rec_simple, spec = spec_lm_pen)
system.time(wf1_tune <- wf1 %>% tune_grid(resamples = rides_folds))
```

```{r}
show_best(wf1_tune, metric="rmse")
```


```{r}
autoplot(wf1_tune)
```


## Ex 5: Racing

Repeat Ex 4 but use a grid size of 25 and racing to reduced the amount of time (how much time does it take?)

I recommend leaving `save_pred = FALSE` and `save_workflow = FALSE` (these are the defaults).  This is contrary to some of the scripts in the book, but I think Rstudio stays happier with these settings.

Plot the results and also print a table of the best models

```{r}
race_ctrl <- control_race(save_pred = FALSE, parallel_over = "everything")
system.time(wf1_race <- wf1 %>% tune_race_anova(resamples = rides_folds, grid = 25, control=race_ctrl))
```

```{r}
show_best(wf1_race, metric = "rmse")
```


```{r}
autoplot(wf1_race)
```

## Exercise 6 workflow set, tuning

Now create a workflow set that uses the `rec_simple` recipe and both two model specifications. Tune them using racing.  Plot the results and print summaries of the best.  Did the penalized regression or the random forests provide a better fit?  

```{r}
wfs1 <- workflow_set(preproc = list(simple=rec_simple), models = list(pen_lm=spec_lm_pen, rf=spec_rf))

system.time(wfs1_race <- wfs1 %>% workflow_map("tune_race_anova", control=race_ctrl, grid=25, resamples=rides_folds))
```
time was 137 sec on "smaller" data set

```{r}
wfs1_race$result[[2]]$.metrics[[1]]
```


```{r}
rank_results(wfs1_race, rank_metric = "rmse", select_best = TRUE)
```

```{r}
autoplot(
   wfs1_race,
   rank_metric = "rmse",  
   metric = "rmse",       
   select_best = TRUE    
) +
   geom_text(aes(y = mean - 1.5, label = wflow_id), angle = 90, hjust = 1) +
   lims(y = c(10, 18)) +
   theme(legend.position = "none")
```
## Exercise 7
Can you figure out how to extract and plot/summarize the data for just the random forest spec?  (I want output analagous to what you did for Exercise 5)

## Exercise 8

Using the results from Exercise 6 as a starting point, use a Bayesian tune to see if you can further improve the random forest fit.

Hint: you will neeed to use `extract_parameter_set_dials(spec_rf) %>%
  finalize(rides_train)`  to create a parameter set to feed into the Bayesian tuning function

```{r}
wfs1_race$result[[2]] %>% autoplot()
```

```{r}
wfs1_race$result[[1]]$.metrics[[2]]
```


```{r}
ctrl_bayes <- control_bayes(verbose = TRUE)
rf_params <- extract_parameter_set_dials(spec_rf) %>%
  finalize(rides_train)
rf_wf <- workflow(preprocessor = rec_simple, spec=spec_rf)
rf_bo <-
  rf_wf %>%
  tune_bayes(
    resamples = rides_folds,
    initial = wfs1_race$result[[2]],
    iter = 25,
    param_info = rf_params,
    control = ctrl_bayes
  )
```

```{r}
show_best(rf_bo)
```


## Exercise 9
okay, now that we have reviewed how this works, fit as many extra models as you would like to come up with the best predictive fit that you can.  See Chapter 15 for a bunch of possibilities.  You may want to do these as a workflow set (or sets) to make comparison easier.

### An additional recipe
```{r}
rec_poly <- 
   rec_normal %>% 
   step_poly(all_predictors()) %>% 
   step_interact(~ all_predictors():all_predictors())
```

### Create some model specifications
```{r}
library(rules)
library(baguette)

spec_nnet <- 
   mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>% 
   set_engine("nnet", MaxNWts = 2600) %>% 
   set_mode("regression")

spec_mars <- 
   mars(prod_degree = tune()) %>%  #<- use GCV to choose terms
   set_engine("earth") %>% 
   set_mode("regression")

spec_svm_r <- 
   svm_rbf(cost = tune(), rbf_sigma = tune()) %>% 
   set_engine("kernlab") %>% 
   set_mode("regression")

spec_svm_p <- 
   svm_poly(cost = tune(), degree = tune()) %>% 
   set_engine("kernlab") %>% 
   set_mode("regression")

spec_knn <- 
   nearest_neighbor(neighbors = tune(), dist_power = tune(), weight_func = tune()) %>% 
   set_engine("kknn") %>% 
   set_mode("regression")

spec_cart <- 
   decision_tree(cost_complexity = tune(), min_n = tune()) %>% 
   set_engine("rpart") %>% 
   set_mode("regression")

spec_bag_cart <- 
   bag_tree() %>% 
   set_engine("rpart", times = 50L) %>% 
   set_mode("regression")

spec_rf <- 
   rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
   set_engine("ranger") %>% 
   set_mode("regression")

spec_xgb <- 
   boost_tree(tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(), 
              min_n = tune(), sample_size = tune(), trees = tune()) %>% 
   set_engine("xgboost") %>% 
   set_mode("regression")

spec_cubist <- 
   cubist_rules(committees = tune(), neighbors = tune()) %>% 
   set_engine("Cubist") 
```

```{r}
nnet_param <- 
   spec_nnet %>% 
   extract_parameter_set_dials() %>% 
   update(hidden_units = hidden_units(c(1, 27)))
```


### combine recipes and model specifications into worflow sets

```{r}
normalized <- 
   workflow_set(
      preproc = list(normalized = rec_normal), 
      models = list(SVM_radial = spec_svm_r, SVM_poly = spec_svm_p, 
                    KNN = spec_knn, neural_network = spec_nnet)
   )

normalized <- 
   normalized %>% 
   option_add(param_info = nnet_param, id = "normalized_neural_network")

normalized

```

models using the unmodified predictors
```{r}
no_pre_proc <- 
   workflow_set(
      preproc = list(simple = rec_simple), 
      models = list(MARS = spec_mars, CART = spec_cart, CART_bagged = spec_bag_cart,
                    RF = spec_rf, boosting = spec_xgb, Cubist = spec_cubist)
   )
no_pre_proc
```

models using the polynomial and interactions:
```{r}
with_features <- 
   workflow_set(
      preproc = list(full_quad = rec_poly), 
      models = list(linear_reg = spec_lm_pen, KNN = spec_knn)
   )
```

bring it all together
```{r}
all_workflows <- 
   bind_rows(no_pre_proc, normalized, with_features) %>% 
   # Make the workflow ID's a little more simple: 
   mutate(wflow_id = gsub("(simple_)|(normalized_)", "", wflow_id))
all_workflows
```

## fit the models, using a racing approach

```{r}
race_ctrl <-
   control_race(
      save_pred = FALSE,
      parallel_over = "everything",
      save_workflow = FALSE
   )
```


```{r}
system.time(simple_results <-
   no_pre_proc %>%
   workflow_map(
      "tune_race_anova",
      seed = 707,
      resamples = rides_folds,
      grid = 25,
      control = race_ctrl
   ))
```
12 minutes

```{r}
system.time(norm_results <-
   normalized %>%
   workflow_map(
      "tune_race_anova",
      seed = 707,
      resamples = rides_folds,
      grid = 25,
      control = race_ctrl
   ))
```

2 hours, 15 minutes

```{r}
system.time(feature_results <-
   with_features %>%
   workflow_map(
      "tune_race_anova",
      seed = 707,
      resamples = rides_folds,
      grid = 25,
      control = race_ctrl
   ))
```

80 minutes

bring the results together...

```{r}
all_results <- bind_rows(simple_results, norm_results, feature_results)
```

```{r}
all_results %>% 
   rank_results(rank_metric = "rsq") %>% 
  # filter(.metric == "rsq") %>% 
   select(wflow_id, model, .config, rsq = mean, rank)
```

```{r}
autoplot(
   all_results,
   rank_metric = "rsq",  # <- how to order models
   metric = "rsq",       # <- which metric to visualize
   select_best = TRUE     # <- one point per workflow
) +
   geom_text(aes(y = mean - .1, label = wflow_id), angle = 90, hjust = 1) +
   lims(y = c(0, 0.6)) +
   theme(legend.position = "none")
```

```{r}
save(all_results, rides, rides_folds, rides_test, rides_train, rides_split, file="TMWR_wrapup_all_race_results.Rdata")
```

## Exercise 10

get the best model, fit to the test set, check metrics, and plot observed versus predicted

```{r}
best_results <- 
   all_results %>% 
   extract_workflow_set_result("simple_boosting") %>% 
   select_best(metric = "rmse")
best_results

boosting_test_results <- 
   all_results %>% 
   extract_workflow("simple_boosting") %>% 
   finalize_workflow(best_results) %>% 
   last_fit(split = rides_split)
```

```{r}
collect_metrics(boosting_test_results)

```

```{r}
boosting_test_results %>% 
   collect_predictions() %>% 
   ggplot(aes(x = heart_rate, y = .pred)) + 
   geom_abline(color = "gray50", lty = 2) + 
   geom_point(alpha = 0.1) + 
   coord_obs_pred() + 
   labs(x = "observed", y = "predicted")
```



## Trying to improve with Bayes (no real improvement)
```{r}
boost_wflow <- workflow(preprocessor = rec_simple, spec = spec_xgb)

bayes_tune_boost <- tune_bayes(boost_wflow, resamples = rides_folds, initial=25, control=ctrl_bayes)
```
```{r}
show_best(bayes_tune_boost, metric = "rsq")
```

### race results saving workflow, etc

for some downstream analysis, need to have workflow and model predictions saved in the race results.  Do that here:

```{r}
race_ctrl_save <-
   control_race(
     verbose = TRUE,
     verbose_elim = TRUE,
      save_pred = TRUE,
      parallel_over = "everything",
      save_workflow = TRUE
   )

system.time(all_race_results_save <-
   all_workflows %>%
   workflow_map(
      "tune_race_anova",
      seed = 707,
      resamples = rides_folds,
      grid = 25,
      control = race_ctrl_save
   ))

save(all_race_results_save, rides, rides_folds, rides_test, rides_train, rides_split, file="TMWR_wrapup_all_race_results_save.Rdata")
```

